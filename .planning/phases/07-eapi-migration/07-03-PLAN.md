---
phase: 07-eapi-migration
plan: 03
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - lib/term_tools.py
  - lib/author_tools.py
  - lib/booklist_tools.py
  - lib/enhanced_metadata.py
autonomous: true

must_haves:
  truths:
    - "Term search uses EAPI search endpoint instead of HTML scraping"
    - "Author search uses EAPI search endpoint instead of HTML scraping"
    - "Enhanced metadata uses EAPI /book/{id}/{hash} instead of HTML parsing"
    - "Booklist tools use EAPI or gracefully degrade"
  artifacts:
    - path: "lib/term_tools.py"
      provides: "EAPI-backed term search"
    - path: "lib/author_tools.py"
      provides: "EAPI-backed author search"
    - path: "lib/enhanced_metadata.py"
      provides: "EAPI-backed metadata extraction"
    - path: "lib/booklist_tools.py"
      provides: "EAPI-backed or degraded booklist fetching"
  key_links:
    - from: "lib/term_tools.py"
      to: "zlibrary/src/zlibrary/eapi.py"
      via: "EAPIClient.search() calls"
      pattern: "eapi.*search"
    - from: "lib/enhanced_metadata.py"
      to: "zlibrary/src/zlibrary/eapi.py"
      via: "EAPIClient.get_book_info() calls"
      pattern: "eapi.*get_book_info"
---

<objective>
Replace HTML scraping in the four lib/ tool modules with EAPI calls. These modules currently do their own HTTP requests + BeautifulSoup parsing independently of the vendored fork.

Purpose: Eliminates the remaining Cloudflare-blocked HTML requests in the tool layer, completing the EAPI migration for all search/metadata operations.
Output: All lib/ tools use EAPIClient for Z-Library data access.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-eapi-migration/07-RESEARCH.md
@.planning/phases/07-eapi-migration/07-01-SUMMARY.md
@lib/term_tools.py
@lib/author_tools.py
@lib/booklist_tools.py
@lib/enhanced_metadata.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate term_tools.py and author_tools.py to EAPI</name>
  <files>lib/term_tools.py, lib/author_tools.py</files>
  <action>
1. **`term_tools.py`**: Currently constructs a URL like `/s/{term}?e=1`, does HTTP GET, parses `z-bookcard` HTML elements with BeautifulSoup.
   - Replace with: Accept an `EAPIClient` instance (or create one from auth context). Call `eapi_client.search(term, exact=True, **filters)`. Apply `normalize_eapi_search_response()`.
   - Remove `construct_term_search_url()` (no longer needed) or keep as deprecated.
   - Remove all BeautifulSoup imports and HTML parsing (`parse_book_cards`, `extract_book_from_card`, etc.).
   - Update `search_by_term()` main function to accept `eapi_client` parameter instead of raw HTTP session. If backward compat with python_bridge is needed, add optional parameter with fallback.
   - Preserve return format: list of book dicts with same field names.

2. **`author_tools.py`**: Currently constructs author search URL, does HTTP GET, parses HTML book cards.
   - Replace with: Call `eapi_client.search(author_query, **filters)`. The EAPI search endpoint handles author queries naturally.
   - Remove HTML parsing code, BeautifulSoup imports.
   - Update `search_by_author()` to accept `eapi_client` parameter.
   - Preserve `validate_author_name()` and `normalize_author_query()` — these are still useful for input validation.
   - Preserve return format.

IMPORTANT: Both modules are called from `python_bridge.py`. Check how python_bridge calls them and ensure the interface change (adding eapi_client param) is compatible. The bridge will be updated in Plan 04 to pass the client.
  </action>
  <verify>
`grep -c 'BeautifulSoup' lib/term_tools.py lib/author_tools.py` → 0 for both files.
`python -c "from lib.term_tools import search_by_term; print('OK')"` — import works.
  </verify>
  <done>term_tools.py and author_tools.py use EAPI for all searches. No BeautifulSoup HTML parsing remains.</done>
</task>

<task type="auto">
  <name>Task 2: Migrate booklist_tools.py and enhanced_metadata.py to EAPI</name>
  <files>lib/booklist_tools.py, lib/enhanced_metadata.py</files>
  <action>
1. **`enhanced_metadata.py`**: Currently fetches book detail HTML pages, parses with BeautifulSoup to extract description, terms, booklists, ratings, IPFS CIDs, categories, ISBNs.
   - Replace with: Call `eapi_client.get_book_info(book_id, book_hash)` which returns all metadata as JSON.
   - Map EAPI metadata fields to existing return format.
   - The EAPI response likely includes: title, author, year, publisher, language, pages, isbn, cover, description, categories, etc.
   - For fields the EAPI may NOT include (terms, booklists, IPFS CIDs): Check the EAPI response first. If missing, either mark as "not available via EAPI" or check if there's an alternative endpoint.
   - Remove all BeautifulSoup parsing functions (`extract_description`, `extract_terms`, `extract_booklists`, `extract_ratings`, `extract_ipfs`, etc.) — replace with simple JSON field access.
   - The `get_book_metadata_complete()` function in python_bridge.py calls this module — ensure the return dict has the same keys.

2. **`booklist_tools.py`**: Currently constructs booklist URLs, fetches HTML, parses book cards.
   - Per research: EAPI may not have a direct booklist browsing endpoint.
   - Strategy: Use `eapi_client.search()` as a fallback for booklist content. If a booklist ID/hash can be looked up via EAPI, use that. Otherwise, mark `fetch_booklist` as degraded with a clear message: "Booklist browsing unavailable — use search_books instead."
   - Remove HTML parsing code and BeautifulSoup imports.
   - Preserve the `fetch_booklist()` function signature but return a degraded response with explanation if EAPI doesn't support it.

IMPORTANT: enhanced_metadata.py has 20+ BeautifulSoup usages — this is a substantial rewrite. Focus on the `get_enhanced_metadata()` or equivalent main function and ensure it returns the same dict structure from EAPI JSON.
  </action>
  <verify>
`grep -c 'BeautifulSoup' lib/booklist_tools.py lib/enhanced_metadata.py` → 0 for both.
`python -c "from lib.enhanced_metadata import extract_description; print('OK')"` or equivalent main function import works.
  </verify>
  <done>booklist_tools.py and enhanced_metadata.py use EAPI. No BeautifulSoup HTML parsing in any lib/ tool module.</done>
</task>

</tasks>

<verification>
1. `grep -r 'BeautifulSoup' lib/term_tools.py lib/author_tools.py lib/booklist_tools.py lib/enhanced_metadata.py` — zero matches
2. `grep -r 'z-bookcard' lib/` — zero matches (HTML element no longer parsed)
3. All four files import cleanly
4. `uv run pytest __tests__/python/ -x -q` — check for regressions (some mocks may need updates in Plan 04)
</verification>

<success_criteria>
- All 4 lib/ tool modules use EAPI calls instead of HTML scraping
- No BeautifulSoup imports remain in these files
- Return formats preserved for python_bridge compatibility
- Booklist functionality gracefully degraded if EAPI doesn't support it
</success_criteria>

<output>
After completion, create `.planning/phases/07-eapi-migration/07-03-SUMMARY.md`
</output>
