---
phase: 11-body-text-purity
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - __tests__/python/test_recall_baseline.py
  - test_files/ground_truth/body_text_baseline.json
autonomous: true

must_haves:
  truths:
    - "Recall baseline snapshot captures current process_pdf() body text output for all existing test PDFs"
    - "Baseline is frozen as ground truth JSON that future recall regression tests compare against"
    - "Recall regression test asserts no body text is lost relative to baseline"
  artifacts:
    - path: "test_files/ground_truth/body_text_baseline.json"
      provides: "Frozen body text output from process_pdf() for each test PDF"
      contains: "body_text"
    - path: "__tests__/python/test_recall_baseline.py"
      provides: "Recall regression test comparing current output to baseline"
      contains: "test_no_body_text_recall_loss"
  key_links:
    - from: "__tests__/python/test_recall_baseline.py"
      to: "test_files/ground_truth/body_text_baseline.json"
      via: "loads baseline JSON and compares"
      pattern: "body_text_baseline"
---

<objective>
Create a recall regression baseline by snapshotting current process_pdf() output for all test PDFs, then building a test that ensures no body text is lost during Phase 11 refactoring.

Purpose: CONTEXT.md mandates zero tolerance for recall loss. This baseline must exist BEFORE any refactoring to detect regressions.
Output: Ground truth baseline JSON + recall regression pytest
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-body-text-purity/11-CONTEXT.md
@lib/rag/orchestrator_pdf.py
@test_files/
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate recall baseline snapshot</name>
  <files>test_files/ground_truth/body_text_baseline.json</files>
  <action>
Create a script (can be inline in a test or temporary) that:

1. Finds all PDF files in `test_files/` (excluding subdirectories like `ground_truth/`)
2. Runs `process_pdf(pdf_path, output_format="markdown")` on each
3. Captures the FULL body text output string for each PDF
4. Saves as `test_files/ground_truth/body_text_baseline.json` with structure:
```json
{
  "generated": "2026-02-02",
  "description": "Recall baseline: process_pdf() body text output before Phase 11 refactoring",
  "baselines": {
    "derrida_grammatology.pdf": {
      "body_text_length": 12345,
      "body_text_hash": "sha256:abc...",
      "line_count": 234,
      "sample_lines": ["first 3 lines...", "...", "..."]
    },
    ...
  }
}
```

For each PDF, store: filename, body_text_length (char count), body_text_hash (SHA-256 of full text), line_count, and first 3 non-empty lines as sample_lines.

The hash is the recall anchor — if body text changes, the hash changes, and the test must verify no lines were LOST (additions are OK).

Also store the full body text in a separate file per PDF: `test_files/ground_truth/baseline_texts/{filename}.txt` for detailed diff analysis when recall fails.

Handle PDFs that fail to process gracefully (log warning, skip).
  </action>
  <verify>
```bash
cd /home/rookslog/workspace/projects/zlibrary-mcp
test -f test_files/ground_truth/body_text_baseline.json && echo "Baseline JSON exists" || echo "MISSING"
uv run python -c "
import json
with open('test_files/ground_truth/body_text_baseline.json') as f:
    data = json.load(f)
assert 'baselines' in data
assert len(data['baselines']) > 0
print(f'Baseline covers {len(data[\"baselines\"])} PDFs')
for name, info in list(data['baselines'].items())[:3]:
    print(f'  {name}: {info[\"body_text_length\"]} chars, {info[\"line_count\"]} lines')
"
```
  </verify>
  <done>Baseline JSON exists with hash + length for every processable test PDF; baseline text files saved</done>
</task>

<task type="auto">
  <name>Task 2: Create recall regression test</name>
  <files>__tests__/python/test_recall_baseline.py</files>
  <action>
Create `__tests__/python/test_recall_baseline.py` with:

1. `test_no_body_text_recall_loss()`:
   - Loads `test_files/ground_truth/body_text_baseline.json`
   - For each PDF in baseline, runs `process_pdf()` on it
   - Extracts lines from current output and baseline text file
   - Asserts every line in the baseline text is present in current output (recall check)
   - Lines may appear in different order or with different whitespace — normalize before comparison
   - New lines in current output are OK (precision loss is acceptable per CONTEXT.md)
   - Reports which specific lines were lost if test fails

2. `test_body_text_not_shorter()`:
   - Quick sanity check: current body text length >= 95% of baseline length for each PDF
   - Catches gross recall loss without line-by-line comparison

3. Helper: `normalize_line(line: str) -> str` that strips whitespace, lowercases, and removes repeated spaces for fuzzy matching.

Use `@pytest.mark.slow` marker for the full recall test (processes all PDFs). Use `@pytest.mark.parametrize` over the baseline PDFs.

Skip PDFs that no longer exist in test_files/ (graceful handling of test corpus changes).
  </action>
  <verify>
```bash
cd /home/rookslog/workspace/projects/zlibrary-mcp
uv run pytest __tests__/python/test_recall_baseline.py -v --tb=short 2>&1 | tail -20
```
  </verify>
  <done>Recall regression tests pass against current baseline; any future body text loss will be caught</done>
</task>

</tasks>

<verification>
```bash
cd /home/rookslog/workspace/projects/zlibrary-mcp
uv run pytest __tests__/python/test_recall_baseline.py -v
```
</verification>

<success_criteria>
- Baseline JSON covers all processable test PDFs with hashes and line counts
- Recall regression test passes (current output matches baseline)
- Test will fail if any body text line is lost during Phase 11 refactoring
- Additions to body text (precision changes) do not cause test failure
</success_criteria>

<output>
After completion, create `.planning/phases/11-body-text-purity/11-02-SUMMARY.md`
</output>
