# RAG Output Quality Evaluation Specification

**Version:** 1.0
**Date:** 2025-04-29
**Author:** Specification Writer (AI Mode)

## 1. Introduction

This document defines the quality evaluation criteria for the Markdown and Text output generated by the `process_document_for_rag` tool. This tool processes EPUB (using `ebooklib`) and PDF (using `PyMuPDF`) files to produce formats optimized for Retrieval-Augmented Generation (RAG) ingestion. These criteria will be used by the `qa-tester` mode to assess the tool's output quality.

## 2. Evaluation Scope

- **Input Formats:** EPUB, PDF
- **Output Formats:** Markdown (`.md`), Plain Text (`.txt`)
- **Tool:** `process_document_for_rag` (implemented in `lib/python_bridge.py`)

## 3. General Principles

- **Maximize Fidelity:** Extract as much meaningful content as possible while preserving the original intent and structure.
- **Minimize Noise:** Remove or clearly demarcate non-essential elements (e.g., headers/footers, complex formatting artifacts) that could hinder RAG performance.
- **Optimize for Chunking:** Structure the output (especially Markdown) to facilitate logical splitting into meaningful chunks for vectorization and retrieval.
- **Consistency:** Apply formatting rules consistently across documents and formats.

## 4. Markdown Output Quality Criteria (`.md`)

### 4.1. Structure Preservation

- **Criterion:** Accuracy of mapping original document structure (chapters, sections, subsections) to Markdown headings (H1-H6).
- **Measurement:**
    - Percentage of headings correctly identified and mapped to the appropriate level (H1-H6).
    - Correct nesting of headings.
    - Visual inspection score (1-5) for overall structural representation.
- **TDD Anchor:** Test parsing of known EPUB/PDF structures (e.g., ToC, nested sections) and verify correct Markdown heading generation.

### 4.2. Content Fidelity

- **Criterion:** Accurate extraction of core textual content and appropriate representation of common elements.
- **Measurement:**
    - **Text:** Percentage of core text accurately extracted (e.g., using BLEU score or manual comparison on samples).
    - **Lists:** Correct conversion to Markdown ordered (`1.`, `2.`) or unordered (`*`, `-`) lists, including nesting. Percentage of lists correctly formatted.
    - **Tables:** Representation of tables (e.g., Markdown table syntax, simplified text, placeholder). Consistency of representation. Percentage of tables captured.
    - **Code Blocks:** Correct identification and formatting using fenced code blocks (```) with language hints if possible. Percentage of code blocks correctly formatted.
    - **Blockquotes:** Correct identification and formatting using `>`. Percentage of blockquotes correctly formatted.
- **TDD Anchor:** Test extraction of specific list, table, code block, and blockquote examples from test documents. Verify text accuracy for selected paragraphs.

### 4.3. References Handling

- **Criterion:** Consistent and identifiable formatting of footnotes, endnotes, citations, and bibliographies.
- **Measurement:**
    - Percentage of references (footnotes, endnotes, citations) captured.
    - Consistency of formatting (e.g., Markdown footnote syntax `[^1]`, inline markers, separate section).
    - Identifiability/Linkage: Can reference markers be easily associated with their content?
- **TDD Anchor:** Test extraction and formatting of footnotes/endnotes from test documents. Verify consistency of reference marker style.

### 4.4. RAG Suitability

- **Criterion:** Output format facilitates effective chunking and potential metadata extraction for RAG.
- **Measurement:**
    - **Chunking:** Clarity of section/paragraph breaks. Use of horizontal rules (`---`) or extra newlines to demarcate potential chunk boundaries. Qualitative score (1-5) for ease of automated chunking.
    - **Metadata:** Association of text with its nearest preceding heading (potential metadata). Presence of document title/author if extractable. Percentage of paragraphs correctly associated with a heading.
    - **Images/Figures:** Handling of images (e.g., placeholder text `[Image: description]`, inclusion of alt text/captions if available, removal). Consistency of handling.
- **TDD Anchor:** Test generation of image placeholders with alt text/captions. Test association of paragraph text with the correct preceding Markdown heading. Verify clear demarcation between sections.

## 5. Text Output Quality Criteria (`.txt`)

### 5.1. Content Fidelity

- **Criterion:** Accurate extraction of raw text content, especially from complex layouts.
- **Measurement:**
    - Percentage of core text accurately extracted compared to the source (manual comparison or automated metrics on samples), particularly focusing on multi-column layouts, text within tables, and text flow around images in PDFs.
- **TDD Anchor:** Test text extraction accuracy on a PDF with multi-column layout and embedded tables.

### 5.2. Formatting Preservation

- **Criterion:** Preservation of basic formatting for readability.
- **Measurement:**
    - **Paragraph Breaks:** Accuracy of preserving paragraph breaks (e.g., represented by double newlines). Percentage of original paragraph breaks correctly represented.
    - **Line Breaks:** Handling of line breaks within paragraphs (e.g., preserved, removed). Consistency of handling.
    - **Readability:** Qualitative score (1-5) for overall readability of the plain text output.
- **TDD Anchor:** Test preservation of paragraph breaks (double newlines) and handling of intra-paragraph line breaks.

### 5.3. RAG Suitability

- **Criterion:** Suitability of the plain text output for basic RAG ingestion.
- **Measurement:**
    - **Noise Level:** Amount of extraneous characters, formatting artifacts, or repeated elements (e.g., headers/footers). Qualitative score (1-5, lower is better).
    - **Text Flow:** Logical coherence and ordering of extracted text, especially from complex PDF layouts. Qualitative score (1-5).
- **TDD Anchor:** Test extraction from a PDF with headers/footers to verify their removal or demarcation. Test text ordering from a multi-column PDF.

## 6. Evaluation Process (Guidance for QA Tester)

1.  Select a diverse set of test documents (EPUB and PDF) representing various structures, complexities, and content types (e.g., simple text, academic papers with references, code documentation, multi-column layouts).
2.  Run `process_document_for_rag` on each test document to generate both Markdown and Text outputs.
3.  Evaluate each output file against the corresponding criteria defined in sections 4 and 5.
4.  Record measurements and qualitative scores systematically.
5.  Document specific examples of successes and failures for each criterion.
6.  Summarize findings and identify areas for improvement in the `process_document_for_rag` tool.