---
phase: 14-test-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pytest.ini
  - __tests__/python/test_adaptive_integration.py
  - __tests__/python/test_annas_adapter.py
  - __tests__/python/test_author_tools.py
  - __tests__/python/test_booklist_tools.py
  - __tests__/python/test_compositor.py
  - __tests__/python/test_eapi_download.py
  - __tests__/python/test_enhanced_metadata.py
  - __tests__/python/test_filename_utils.py
  - __tests__/python/test_footnote_continuation.py
  - __tests__/python/test_formatting_group_merger.py
  - __tests__/python/test_garbled_performance.py
  - __tests__/python/test_garbled_text_detection.py
  - __tests__/python/test_inline_footnotes.py
  - __tests__/python/test_libgen_adapter.py
  - __tests__/python/test_margin_detection.py
  - __tests__/python/test_margin_integration.py
  - __tests__/python/test_metadata_generator.py
  - __tests__/python/test_note_classification.py
  - __tests__/python/test_ocr_quality.py
  - __tests__/python/test_performance_footnote_features.py
  - __tests__/python/test_phase_2_integration.py
  - __tests__/python/test_pipeline_integration.py
  - __tests__/python/test_publisher_extraction.py
  - __tests__/python/test_python_bridge.py
  - __tests__/python/test_quality_pipeline_integration.py
  - __tests__/python/test_rag_data_models.py
  - __tests__/python/test_rag_enhancements.py
  - __tests__/python/test_rag_processing.py
  - __tests__/python/test_real_footnotes.py
  - __tests__/python/test_real_world_validation.py
  - __tests__/python/test_recall_baseline.py
  - __tests__/python/test_resolution_analyzer.py
  - __tests__/python/test_resolution_renderer.py
  - __tests__/python/test_run_rag_tests.py
  - __tests__/python/test_source_router.py
  - __tests__/python/test_superscript_detection.py
  - __tests__/python/test_term_tools.py
  - __tests__/python/test_toc_hybrid.py
  - __tests__/python/integration/test_real_zlibrary.py
autonomous: true

must_haves:
  truths:
    - "pytest.ini registers all 7 markers (unit, integration, slow, ground_truth, real_world, performance, e2e) with strict_markers enabled"
    - "Every Python test file under __tests__/python/ declares at least one pytestmark"
    - "Running `uv run pytest -m 'not slow and not integration'` selects only fast tests and completes under 30 seconds"
    - "Running `uv run pytest --strict-markers --co` collects all tests without error (no unregistered markers)"
  artifacts:
    - path: "pytest.ini"
      provides: "Complete marker taxonomy with strict enforcement"
      contains: "strict_markers = true"
    - path: "__tests__/python/test_filename_utils.py"
      provides: "Example unit-marked test file"
      contains: "pytestmark"
  key_links:
    - from: "pytest.ini"
      to: "all test files"
      via: "strict_markers enforcement"
      pattern: "strict_markers = true"
---

<objective>
Register the complete 7-marker pytest taxonomy and apply markers to every Python test file.

Purpose: Enable marker-based test selection so CI can run fast tests separately and developers can target specific test categories. With strict_markers enabled, any future test file without a registered marker will fail at collection time, preventing drift.

Output: Updated pytest.ini with all 7 markers and strict enforcement. Every test file under `__tests__/python/` has a `pytestmark` module-level declaration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary-standard.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-test-infrastructure/14-RESEARCH.md
@pytest.ini
</context>

<tasks>

<task type="auto">
  <name>Task 1: Register complete marker taxonomy in pytest.ini</name>
  <files>pytest.ini</files>
  <action>
Update `pytest.ini` to:

1. Add two missing markers: `unit` and `ground_truth`
2. Keep existing 5 markers (integration, e2e, performance, real_world, slow)
3. Enable `strict_markers = true`

The final `pytest.ini` should be:
```ini
[pytest]
pythonpath = . lib
asyncio_mode = auto
testpaths = __tests__/python
strict_markers = true
markers =
    unit: pure unit tests with mocked dependencies, no file I/O on real test data
    integration: tests requiring real Z-Library credentials or network access (deselect with '-m "not integration"')
    slow: tests processing real PDF/EPUB files via PyMuPDF (deselect with '-m "not slow"')
    ground_truth: tests validating output against ground truth JSON files (deselect with '-m "not ground_truth"')
    real_world: end-to-end validation with real-world documents (deselect with '-m "not real_world"')
    performance: benchmark and timing budget tests (deselect with '-m "not performance"')
    e2e: full MCP pipeline end-to-end tests (deselect with '-m "not e2e"')
```

Do NOT change `pythonpath`, `asyncio_mode`, or `testpaths`.
  </action>
  <verify>
Run: `uv run pytest --strict-markers --co -q 2>&1 | tail -5`
This will FAIL because test files don't have markers yet (expected). But confirm the markers themselves are registered by checking no "Unknown marker" errors for the 7 registered names.
  </verify>
  <done>pytest.ini has all 7 markers registered and strict_markers = true enabled.</done>
</task>

<task type="auto">
  <name>Task 2: Apply pytestmark to all Python test files</name>
  <files>
    __tests__/python/test_adaptive_integration.py
    __tests__/python/test_annas_adapter.py
    __tests__/python/test_author_tools.py
    __tests__/python/test_booklist_tools.py
    __tests__/python/test_compositor.py
    __tests__/python/test_eapi_download.py
    __tests__/python/test_enhanced_metadata.py
    __tests__/python/test_filename_utils.py
    __tests__/python/test_footnote_continuation.py
    __tests__/python/test_formatting_group_merger.py
    __tests__/python/test_garbled_performance.py
    __tests__/python/test_garbled_text_detection.py
    __tests__/python/test_inline_footnotes.py
    __tests__/python/test_libgen_adapter.py
    __tests__/python/test_margin_detection.py
    __tests__/python/test_margin_integration.py
    __tests__/python/test_metadata_generator.py
    __tests__/python/test_note_classification.py
    __tests__/python/test_ocr_quality.py
    __tests__/python/test_performance_footnote_features.py
    __tests__/python/test_phase_2_integration.py
    __tests__/python/test_pipeline_integration.py
    __tests__/python/test_publisher_extraction.py
    __tests__/python/test_python_bridge.py
    __tests__/python/test_quality_pipeline_integration.py
    __tests__/python/test_rag_data_models.py
    __tests__/python/test_rag_enhancements.py
    __tests__/python/test_rag_processing.py
    __tests__/python/test_real_footnotes.py
    __tests__/python/test_real_world_validation.py
    __tests__/python/test_recall_baseline.py
    __tests__/python/test_resolution_analyzer.py
    __tests__/python/test_resolution_renderer.py
    __tests__/python/test_run_rag_tests.py
    __tests__/python/test_source_router.py
    __tests__/python/test_superscript_detection.py
    __tests__/python/test_term_tools.py
    __tests__/python/test_toc_hybrid.py
    __tests__/python/integration/test_real_zlibrary.py
  </files>
  <action>
Add `pytestmark` module-level marker to every test file. Each file needs:
1. Ensure `import pytest` exists (add near top imports if missing)
2. Add `pytestmark = pytest.mark.MARKER` after imports, before first class/function

Use this exact classification (from research findings):

**Mark as `unit` (27 files)** — mocked dependencies, no real file I/O:
```python
pytestmark = pytest.mark.unit
```
Files: test_adaptive_integration.py, test_annas_adapter.py, test_author_tools.py, test_booklist_tools.py, test_compositor.py, test_eapi_download.py, test_enhanced_metadata.py, test_filename_utils.py, test_footnote_continuation.py, test_formatting_group_merger.py, test_garbled_text_detection.py, test_libgen_adapter.py, test_margin_detection.py, test_margin_integration.py, test_metadata_generator.py, test_note_classification.py, test_ocr_quality.py, test_publisher_extraction.py, test_python_bridge.py, test_quality_pipeline_integration.py, test_rag_data_models.py, test_rag_enhancements.py, test_rag_processing.py, test_resolution_analyzer.py, test_resolution_renderer.py, test_run_rag_tests.py, test_source_router.py, test_superscript_detection.py, test_term_tools.py

**Mark as `[slow, ground_truth]`** — processes real PDFs AND loads ground truth:
```python
pytestmark = [pytest.mark.slow, pytest.mark.ground_truth]
```
Files: test_real_footnotes.py, test_recall_baseline.py

**Mark as `[real_world, slow, ground_truth]`** — full real-world validation:
```python
pytestmark = [pytest.mark.real_world, pytest.mark.slow, pytest.mark.ground_truth]
```
Files: test_real_world_validation.py (already has `pytestmark = pytest.mark.real_world` — update to list form)

**Mark as `slow`** — processes real PDFs but no ground truth:
```python
pytestmark = pytest.mark.slow
```
Files: test_inline_footnotes.py, test_toc_hybrid.py, test_phase_2_integration.py

**Mark as `performance`** — timing/benchmark tests:
```python
pytestmark = pytest.mark.performance
```
Files: test_garbled_performance.py, test_performance_footnote_features.py

**Mark as `integration`** — requires Z-Library credentials:
```python
pytestmark = pytest.mark.integration
```
Files: test_pipeline_integration.py (already has `@pytest.mark.integration` on class — convert to module-level pytestmark), integration/test_real_zlibrary.py (already has `@pytest.mark.integration` on class — convert to module-level pytestmark)

**Pattern for adding:** Place `pytestmark` line after the last import statement, separated by one blank line, before any class/function definition. For files that already have per-class/method markers, KEEP those in addition to adding the module-level marker.

For `test_pipeline_integration.py` and `integration/test_real_zlibrary.py`: Add the module-level `pytestmark` but do NOT remove the existing `@pytest.mark.integration` decorators on classes — having both is redundant but harmless and avoids risk.

**Do NOT modify** `conftest.py` or `__init__.py` files — they are not test files and don't need markers.
  </action>
  <verify>
Run all three checks:
1. `uv run pytest --strict-markers --co -q 2>&1 | tail -5` — should collect all tests with zero errors
2. `uv run pytest -m "not slow and not integration" --co -q 2>&1 | tail -3` — should show reduced test count
3. `time uv run pytest -m "not slow and not integration" -p no:benchmark --tb=short -q` — should complete under 30 seconds
  </verify>
  <done>All test files have pytestmark declarations. strict_markers collection succeeds. Fast subset (`-m "not slow and not integration"`) runs under 30 seconds.</done>
</task>

</tasks>

<verification>
1. `uv run pytest --strict-markers --co -q` — collects all tests with no errors (proves all markers registered and applied)
2. `uv run pytest -m "not slow and not integration" --tb=short -q` — runs fast subset under 30 seconds
3. `grep -rL "pytestmark" __tests__/python/test_*.py` — returns empty (all test files have pytestmark)
4. `uv run pytest` — full suite still passes (no regressions)
</verification>

<success_criteria>
- pytest.ini has 7 markers with strict_markers = true
- Every test file has a pytestmark declaration
- Fast subset completes under 30 seconds
- Full test suite passes without errors
</success_criteria>

<output>
After completion, create `.planning/phases/14-test-infrastructure/14-01-SUMMARY.md`
</output>
