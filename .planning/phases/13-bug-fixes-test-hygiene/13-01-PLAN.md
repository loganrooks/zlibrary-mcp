---
phase: 13-bug-fixes-test-hygiene
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pytest.ini
  - src/lib/paths.ts
  - __tests__/paths.test.js
  - __tests__/integration/mcp-protocol.test.js
  - __tests__/e2e/docker-mcp-e2e.test.js
autonomous: true

must_haves:
  truths:
    - "npm test exits with code 0 and all Jest assertions pass"
    - "uv run pytest collects all test files without collection errors"
    - "uv run pytest --strict-markers succeeds with no unregistered marker warnings"
  artifacts:
    - path: "pytest.ini"
      provides: "Test collection configuration and marker registration"
      contains: "testpaths = __tests__/python"
    - path: "src/lib/paths.ts"
      provides: "Updated path helpers matching UV conventions"
      exports: ["getProjectRoot", "getPythonScriptPath", "getPythonLibDirectory", "getPackageJsonPath", "getPyprojectTomlPath", "getVenvPath"]
    - path: "__tests__/paths.test.js"
      provides: "Tests aligned with current file layout"
    - path: "__tests__/integration/mcp-protocol.test.js"
      provides: "Tool list assertions matching 13 registered tools"
      contains: "search_multi_source"
    - path: "__tests__/e2e/docker-mcp-e2e.test.js"
      provides: "Expected tool count matching 13 registered tools"
      contains: "EXPECTED_TOOL_COUNT = 13"
  key_links:
    - from: "__tests__/paths.test.js"
      to: "dist/lib/paths.js"
      via: "import from compiled output"
      pattern: "from.*dist/lib/paths"
    - from: "__tests__/integration/mcp-protocol.test.js"
      to: "dist/index.js"
      via: "import toolRegistry"
      pattern: "import.*dist/index"
---

<objective>
Fix all test failures in Jest and pytest from stale assertions, missing configuration, and unregistered markers (BUG-01, BUG-02, BUG-03).

Purpose: Green CI is the foundation for v1.2. All 7 test failures (5 Jest + 2 pytest collection errors) stem from code evolving faster than tests. This plan updates tests and configuration to match the current codebase state.

Output: Both `npm test` and `uv run pytest --strict-markers` exit cleanly with zero failures.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary-standard.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-bug-fixes-test-hygiene/13-RESEARCH.md

@pytest.ini
@src/lib/paths.ts
@__tests__/paths.test.js
@__tests__/integration/mcp-protocol.test.js
@__tests__/e2e/docker-mcp-e2e.test.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix pytest configuration (BUG-02 + BUG-03)</name>
  <files>pytest.ini</files>
  <action>
    Update pytest.ini to fix two issues:

    1. **Add testpaths** to restrict test collection to `__tests__/python` only. This prevents pytest from collecting `scripts/test_marginalia_detection.py` and `scripts/archive/test_tesseract_comparison.py` which are utility scripts, not tests. Add the line `testpaths = __tests__/python` after the `asyncio_mode = auto` line.

    2. **Register missing markers** by adding `real_world` and `slow` to the existing markers list:
       - `real_world: marks tests requiring real-world test PDFs and ground truth data (deselect with '-m "not real_world"')`
       - `slow: marks tests that are slow to run (deselect with '-m "not slow"')`

    The final pytest.ini should have: pythonpath, asyncio_mode, testpaths, and markers (integration, e2e, performance, real_world, slow).
  </action>
  <verify>
    Run `uv run pytest --collect-only 2>&1 | head -30` and confirm:
    - No collection errors from scripts/ directory
    - Tests only collected from __tests__/python/

    Run `uv run pytest --strict-markers --collect-only 2>&1 | tail -5` and confirm no "Unknown marker" warnings.
  </verify>
  <done>
    `uv run pytest --strict-markers` exits with code 0 (or only skips due to missing credentials). Zero collection errors. All 5 markers (integration, e2e, performance, real_world, slow) registered.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix Jest test failures (BUG-01)</name>
  <files>
    src/lib/paths.ts
    __tests__/paths.test.js
    __tests__/integration/mcp-protocol.test.js
    __tests__/e2e/docker-mcp-e2e.test.js
  </files>
  <action>
    Fix 5 Jest failures across 3 test suites:

    **A. src/lib/paths.ts -- Update stale path helpers:**
    - Replace `getRequirementsTxtPath()` with `getPyprojectTomlPath()` that returns `path.join(getProjectRoot(), 'pyproject.toml')`. Update JSDoc accordingly. This function is not imported anywhere in production code, only in tests.
    - Update `getVenvPath()` to return `.venv` instead of `venv` (UV convention). Update JSDoc example to show `.venv`.

    **B. __tests__/paths.test.js -- Align tests with current layout:**
    - Update import: replace `getRequirementsTxtPath` with `getPyprojectTomlPath` in the import statement.
    - Remove `client_manager.py` from the scripts array in the "should handle different script names" test (line 75). The array should be: `['python_bridge.py', 'rag_processing.py', 'enhanced_metadata.py']`.
    - Rename the `getRequirementsTxtPath()` describe block to `getPyprojectTomlPath()`. Update the test to check for `pyproject.toml` instead of `requirements.txt`. Verify `existsSync` returns true.
    - In the `getVenvPath()` test, update: the assertion `expect(venvPath).toContain('venv')` still works for `.venv` since `.venv` contains `venv`. No change needed there.
    - In the "Path Resolution Consistency" test, replace `getRequirementsTxtPath` with `getPyprojectTomlPath` and rename the variable from `reqPath` to `tomlPath`.

    **C. __tests__/integration/mcp-protocol.test.js -- Add missing tool:**
    - Add `'search_multi_source'` to the `EXPECTED_TOOLS` array (line 31, after `'search_advanced'`).
    - Also add `'search_multi_source'` to the `READ_ONLY_TOOLS` array since it's a search tool (read-only, no side effects).
    - Update the test "should expose all 11 expected tools" -- the test description says "11" but should say "all expected tools" (the count is derived from the array length, so just fix the description text if it mentions a hardcoded number).
    - Update line 134 comment from "12 tools" to "13 tools".
    - Update line 479 assertion from `expect(Object.keys(toolRegistry).length).toBe(12)` to `.toBe(13)`.

    **D. __tests__/e2e/docker-mcp-e2e.test.js -- Update tool count:**
    - Change `EXPECTED_TOOL_COUNT = 12` to `EXPECTED_TOOL_COUNT = 13` on line 19.

    **IMPORTANT:** After editing paths.ts, you MUST run `npm run build` before running tests. Jest imports from `dist/`, not `src/`.
  </action>
  <verify>
    Run `npm run build && npm test 2>&1 | tail -20` and confirm:
    - All test suites pass (paths.test.js, mcp-protocol.test.js, docker-mcp-e2e.test.js)
    - Zero test failures
    - Exit code 0

    Note: docker-mcp-e2e.test.js spawns the actual server and may fail if the Python venv isn't ready. If that specific suite fails due to server startup (not assertion), that's acceptable -- the assertion fix (12 -> 13) is still correct.
  </verify>
  <done>
    `npm run build && npm test` exits with code 0. All paths.test.js assertions pass (pyproject.toml exists, .venv path correct, no client_manager.py). mcp-protocol.test.js passes with 13 tools including search_multi_source. docker-mcp-e2e.test.js has EXPECTED_TOOL_COUNT = 13.
  </done>
</task>

</tasks>

<verification>
Run the combined verification command:
```bash
npm run build && npm test && uv run pytest --strict-markers
```
Both test runners should exit with code 0.

Additional checks:
- `uv run pytest --collect-only 2>&1 | grep "scripts/"` should return no results
- `grep -c "testpaths" pytest.ini` should return 1
- `grep -c "real_world\|slow" pytest.ini` should return 2
</verification>

<success_criteria>
1. `npm test` exits with code 0, all Jest suites pass
2. `uv run pytest` collects tests only from __tests__/python/, zero collection errors
3. `uv run pytest --strict-markers` succeeds with all markers registered
4. paths.ts exports getPyprojectTomlPath (not getRequirementsTxtPath) and getVenvPath returns .venv path
</success_criteria>

<output>
After completion, create `.planning/phases/13-bug-fixes-test-hygiene/13-01-SUMMARY.md`
</output>
