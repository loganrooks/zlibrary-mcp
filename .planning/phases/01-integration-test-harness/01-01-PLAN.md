---
phase: 01-integration-test-harness
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - __tests__/integration/bridge-tools.test.js
  - __tests__/integration/brk-001-reproduction.test.js
  - __tests__/integration/fixtures/recorded-responses/search.json
  - __tests__/integration/fixtures/recorded-responses/full_text_search.json
  - __tests__/integration/fixtures/recorded-responses/get_download_history.json
  - __tests__/integration/fixtures/recorded-responses/get_download_limits.json
  - __tests__/integration/fixtures/recorded-responses/download_book.json
  - __tests__/integration/fixtures/recorded-responses/process_document.json
  - __tests__/integration/fixtures/recorded-responses/get_book_metadata_complete.json
  - __tests__/integration/fixtures/recorded-responses/search_by_term_bridge.json
  - __tests__/integration/fixtures/recorded-responses/search_by_author_bridge.json
  - __tests__/integration/fixtures/recorded-responses/fetch_booklist_bridge.json
  - __tests__/integration/fixtures/recorded-responses/search_advanced.json
  - package.json
  - ISSUES.md
autonomous: true

must_haves:
  truths:
    - "Running `npm run test:integration` executes a real PythonShell invocation that returns valid JSON from the Python bridge"
    - "Each of the 11 MCP tools has a recorded-mode test verifying response shape"
    - "Live mode (TEST_LIVE=true) tests hit the real Python bridge with actual PythonShell invocation"
    - "BRK-001 reproduction attempt is documented with exact error or confirmed resolved"
  artifacts:
    - path: "__tests__/integration/bridge-tools.test.js"
      provides: "Per-tool integration tests in recorded and live modes"
      min_lines: 100
    - path: "__tests__/integration/brk-001-reproduction.test.js"
      provides: "BRK-001 reproduction test and investigation"
      min_lines: 30
    - path: "__tests__/integration/fixtures/recorded-responses/search.json"
      provides: "Recorded fixture for search tool"
      contains: "content"
  key_links:
    - from: "__tests__/integration/bridge-tools.test.js"
      to: "src/lib/zlibrary-api.ts"
      via: "callPythonFunction or equivalent bridge call"
      pattern: "callPythonFunction|PythonShell"
    - from: "package.json"
      to: "__tests__/integration/"
      via: "test:integration npm script"
      pattern: "test:integration"
---

<objective>
Create integration tests that verify each of the 11 MCP tools works through the real Node.js-to-Python bridge, in both recorded mode (static fixtures, no network) and live mode (real API calls). Reproduce and document BRK-001.

Purpose: Establish the safety net that all subsequent phases depend on — every future change can be validated against this harness.
Output: bridge-tools.test.js (11 tool tests), brk-001-reproduction.test.js, 11 recorded-response fixtures, npm scripts.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-integration-test-harness/01-RESEARCH.md

Key reference files:
@src/lib/zlibrary-api.ts          — Python bridge interface (callPythonFunction)
@src/index.ts                      — MCP tool definitions (11 tools)
@lib/python_bridge.py              — Python bridge entry point
@jest.config.js                    — Existing Jest configuration
@package.json                      — Existing scripts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create recorded-response fixtures and bridge integration tests for all 11 tools</name>
  <files>
    __tests__/integration/bridge-tools.test.js
    __tests__/integration/fixtures/recorded-responses/search.json
    __tests__/integration/fixtures/recorded-responses/full_text_search.json
    __tests__/integration/fixtures/recorded-responses/get_download_history.json
    __tests__/integration/fixtures/recorded-responses/get_download_limits.json
    __tests__/integration/fixtures/recorded-responses/download_book.json
    __tests__/integration/fixtures/recorded-responses/process_document.json
    __tests__/integration/fixtures/recorded-responses/get_book_metadata_complete.json
    __tests__/integration/fixtures/recorded-responses/search_by_term_bridge.json
    __tests__/integration/fixtures/recorded-responses/search_by_author_bridge.json
    __tests__/integration/fixtures/recorded-responses/fetch_booklist_bridge.json
    __tests__/integration/fixtures/recorded-responses/search_advanced.json
    package.json
  </files>
  <action>
1. Read `src/lib/zlibrary-api.ts` to understand the bridge interface — how `callPythonFunction` works, what it returns, and the double-JSON parsing flow (MCP wrapper → inner result).

2. Read `src/index.ts` to confirm the 11 MCP tools and their argument schemas.

3. Create 11 JSON fixture files under `__tests__/integration/fixtures/recorded-responses/`, one per Python bridge function. Each fixture must match the double-JSON MCP response shape:
```json
{
  "content": [
    {
      "type": "text",
      "text": "{\"key\": \"value\"}"  // Inner JSON as string
    }
  ]
}
```
Use realistic field names from the actual tool response shapes.

4. Create `bridge-tools.test.js` with:
   - `TEST_LIVE` env var check (`process.env.TEST_LIVE === 'true'`)
   - A `TOOL_BRIDGE_MAP` constant mapping all 11 tools to their Python bridge function names and minimal arguments (see RESEARCH.md code examples)
   - For each tool: a test that in recorded mode uses `jest.unstable_mockModule()` to mock `python-shell` returning the fixture, and in live mode calls the real bridge
   - Response shape validation: check that parsed result has expected top-level keys
   - Use `describe.each` or a loop over TOOL_BRIDGE_MAP for DRY test generation
   - Timeouts: 10s recorded, 60s live
   - At test suite end: output a matrix summary table (tool × mode → pass/fail) using `afterAll`
   - Important: use `jest.unstable_mockModule()` (NOT `jest.mock()`) for ESM compatibility, with dynamic `import()` after mock setup

5. Add npm scripts to `package.json`:
   - `"test:integration": "node --experimental-vm-modules node_modules/jest/bin/jest.js --testPathPattern 'integration/bridge' --forceExit"`
   - `"test:integration:live": "TEST_LIVE=true npm run test:integration"`

6. Ensure the existing `__tests__/integration/python-bridge-integration.test.js` is not broken by the new test pattern (check path patterns don't conflict).
  </action>
  <verify>
Run `npm run test:integration` — all 11 recorded-mode tests pass. Verify output includes a matrix summary showing 11 tools with PASS status in recorded mode.
  </verify>
  <done>
All 11 MCP tools have recorded-mode integration tests that pass. `npm run test:integration` works as a single command. Fixtures exist for all tools matching the double-JSON MCP response shape.
  </done>
</task>

<task type="auto">
  <name>Task 2: BRK-001 reproduction test and ISSUES.md documentation</name>
  <files>
    __tests__/integration/brk-001-reproduction.test.js
    ISSUES.md
  </files>
  <action>
1. Read the existing `ISSUES.md` to understand current BRK-001 documentation.

2. Read `lib/python_bridge.py` to understand the `download_book` function with `process_for_rag=true` code path.

3. Read the vendored `zlibrary/` fork to find the method that BRK-001 references ("AttributeError when calling missing method in forked zlibrary").

4. Create `brk-001-reproduction.test.js`:
   - Attempt to call the Python bridge `download_book` function with `process_for_rag: true`
   - Use a try/catch — the test passes regardless of outcome (this is investigation, not assertion)
   - On success: log "BRK-001: RESOLVED" with details
   - On failure: log "BRK-001: REPRODUCED" with exact error message, stack trace, and Python stderr
   - Include detailed comments explaining what BRK-001 is and the expected failure mode
   - If live mode is required to reproduce (likely), skip in recorded mode with clear message

5. Run the reproduction test (live mode if credentials available, otherwise document what would be needed).

6. Update `ISSUES.md` with findings:
   - If reproduced: Add exact error, stack trace, root cause analysis, reproduction steps
   - If resolved: Mark as resolved with evidence
   - If cannot reproduce without credentials: Document what was investigated and what would be needed

7. Add inline comments in the test explaining the root cause or investigation status.
  </action>
  <verify>
Run `node --experimental-vm-modules node_modules/jest/bin/jest.js --testPathPattern brk-001 --forceExit` — test executes without crashing (pass regardless of BRK-001 status). Check ISSUES.md has updated BRK-001 entry with reproduction steps or resolution status.
  </verify>
  <done>
BRK-001 status is documented in both a dedicated test file and ISSUES.md. The test captures either the exact error (if reproduced) or confirms resolution. Root cause investigation findings are recorded.
  </done>
</task>

</tasks>

<verification>
1. `npm run test:integration` passes with all 11 tool tests in recorded mode
2. `npm run test:integration:live` runs (may skip/fail tools if no credentials — that's expected)
3. BRK-001 reproduction test executes and documents findings
4. ISSUES.md has updated BRK-001 entry
5. Matrix summary output appears at end of integration test run
</verification>

<success_criteria>
- `npm run test:integration` exits 0 with 11 recorded-mode tool tests passing
- Each tool test validates response shape against the double-JSON MCP format
- BRK-001 documented with reproduction steps or confirmed resolved in ISSUES.md
- No regressions in existing test suite (`npm test` still passes)
</success_criteria>

<output>
After completion, create `.planning/phases/01-integration-test-harness/01-01-SUMMARY.md`
</output>
